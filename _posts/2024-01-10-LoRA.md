---
title: LoRA
date: 2024-01-10 14:10:00 +0800
categories: [PEFT, LoRA]
# tags: [peft]
render_with_liquid: false
---

### **LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS[1]**

#### TL; DR: 一种高效微调(PEFT)方法，用于跨域微调场景，减少训练成本。

#### **解决什么问题：**

- 全量微调(FT)对于LLM成本太大，不利于基础模型对其他域(或下游任务)的适应性，需要一种高效微调的方式提升微调效率，降低微调时间、减小显存占用并保持模型精度；

#### **技术重点：**

- 对于域的迁移，没必要对全量参数进行微调，调整的梯度信息相较于模型参数规模是低秩的(FT over-parameters)，因此可考虑在线性层添加一个紧凑的分支(encoder-decoder)用于**学习跨域的梯度信息**。训练时固定基础模型权重，仅训练低秩的梯度分支。推理时梯度分支输出的梯度信息与基础模型输出进行相加。梯度分支的紧凑程度可通过超参数进行控制。
  
  <center>
      <img style="border-radius: 0.3125em;
      box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
      src="https://github.com/Tsaiyue/tsaiyue.github.io/assets/46399096/d3526dfe-75c9-4cd0-8cab-eb49abcc3404" width = "65%" alt=""/>
      <br>
      <div style="color:orange; border-bottom: 1px solid #d9d9d9;
      display: inline-block;
      color: #999;
      padding: 2px;">
        Fig4: LoRA 跨域梯度信息学习分支与原始全参模型的连接方式
        </div>
  </center>
  
  <center>
      <img style="border-radius: 0.3125em;
      box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
      src="https://github.com/Tsaiyue/tsaiyue.github.io/assets/46399096/489a4c8d-670f-4fa9-80bd-63a80738fa44" width = "65%" alt=""/>
      <br>
      <div style="color:orange; border-bottom: 1px solid #d9d9d9;
      display: inline-block;
      color: #999;
      padding: 2px;">
        Fig4: 前向推理方式
        </div>
  </center>

#### **相关实现：**

- huggingface/diffusers或PaddleMIX/ppdiffusers中包含了相关LoRA处理注意力参数的接口，通过捕获模型对应参数，为其配置LoRA梯度学习分支：
  
  ```python
  # Set correct lora layers
  unet_lora_attn_procs = {}
  for name, attn_processor in unet.attn_processors.items():
      cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
      if name.startswith("mid_block"):
          hidden_size = unet.config.block_out_channels[-1]
      elif name.startswith("up_blocks"):
          block_id = int(name[len("up_blocks.")])
          hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
      elif name.startswith("down_blocks"):
          block_id = int(name[len("down_blocks.")])
          hidden_size = unet.config.block_out_channels[block_id]
  
      unet_lora_attn_procs[name] = LoRAAttnAddedKVProcessor(
          hidden_size=hidden_size,
          cross_attention_dim=cross_attention_dim,
          rank=args.lora_rank,
         )
  
  unet.set_attn_processor(unet_lora_attn_procs)
  unet_lora_layers = AttnProcsLayers(unet.attn_processors)
  ```

- Huggingface/peft提供了更高层的实现，使用`get_peft_model`和 `get_peft_config` 基于LoRA相关config文件进行处理：
  
  ```python
  from transformers import AutoModelForSeq2SeqLM
  from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
  model_name_or_path = "bigscience/mt0-large"
  tokenizer_name_or_path = "bigscience/mt0-large"
  
  peft_config = LoraConfig(
      task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
  ) # r即为秩，以此决定梯度分支的紧致程度
  
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
  model = get_peft_model(model, peft_config)
  model.print_trainable_parameters()
  ```

#### **讨论：**

- 针对Transformer的线性层，但理论上可针对任何神经网络模型；

- 训练微调参数量相较于FT可降低10000倍，但推理由于引入梯度学习分支将带来一定的推理延迟，non-straightforward；

- 可与其他高效微调方式结合，如Prefix-Tuning；

- 关于梯度分支参数的初始化设置，decoder可以用0初始化，使第一轮迭代的时候保持原始模型的输出，提升训练的稳定性，但encoder不能同时为0，否则将无法进行梯度更新，可设置为随机初始化，以此提升多样性.

#### **参考**

- [1] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS, 2021, microsoft, [[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

- [2] [GitHub - huggingface/peft: 🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)


