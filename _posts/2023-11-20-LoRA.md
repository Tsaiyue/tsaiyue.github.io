---
title: LoRA
date: 2023-11-20 14:10:00 +0800
categories: [PEFT, LoRA]
# tags: [peft]
render_with_liquid: false
---

### LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS[1]

#### TL, DL: ä¸€ç§é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•ï¼Œç”¨äºè·¨åŸŸå¾®è°ƒåœºæ™¯ï¼Œå‡å°‘è®­ç»ƒæˆæœ¬ã€‚

#### è§£å†³ä»€ä¹ˆé—®é¢˜ï¼š

- å…¨é‡å¾®è°ƒ(FT)å¯¹äºLLMæˆæœ¬å¤ªå¤§ï¼Œä¸åˆ©äºåŸºç¡€æ¨¡å‹å¯¹å…¶ä»–åŸŸ(æˆ–ä¸‹æ¸¸ä»»åŠ¡)çš„é€‚åº”æ€§ï¼Œéœ€è¦ä¸€ç§é«˜æ•ˆå¾®è°ƒçš„æ–¹å¼æå‡å¾®è°ƒæ•ˆç‡ï¼Œé™ä½å¾®è°ƒæ—¶é—´ã€å‡å°æ˜¾å­˜å ç”¨å¹¶ä¿æŒæ¨¡å‹ç²¾åº¦ï¼›

#### æŠ€æœ¯é‡ç‚¹ï¼š

- å¯¹äºåŸŸçš„è¿ç§»ï¼Œæ²¡å¿…è¦å¯¹å…¨é‡å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´çš„æ¢¯åº¦ä¿¡æ¯ç›¸è¾ƒäºæ¨¡å‹å‚æ•°è§„æ¨¡æ˜¯ä½ç§©çš„(FT over-parameters)ï¼Œå› æ­¤å¯è€ƒè™‘åœ¨çº¿æ€§å±‚æ·»åŠ ä¸€ä¸ªç´§å‡‘çš„åˆ†æ”¯(encoder-decoder)ç”¨äºå­¦ä¹ è·¨åŸŸçš„æ¢¯åº¦ä¿¡æ¯ã€‚è®­ç»ƒæ—¶å›ºå®šåŸºç¡€æ¨¡å‹æƒé‡ï¼Œä»…è®­ç»ƒä½ç§©çš„æ¢¯åº¦åˆ†æ”¯ã€‚æ¨ç†æ—¶æ¢¯åº¦åˆ†æ”¯è¾“å‡ºçš„æ¢¯åº¦ä¿¡æ¯ä¸åŸºç¡€æ¨¡å‹è¾“å‡ºè¿›è¡Œç›¸åŠ ã€‚æ¢¯åº¦åˆ†æ”¯çš„ç´§å‡‘ç¨‹åº¦å¯é€šè¿‡è¶…å‚æ•°è¿›è¡Œæ§åˆ¶ã€‚
  
  <center>
      <img style="border-radius: 0.3125em;
      box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
      src="https://github.com/Tsaiyue/tsaiyue.github.io/assets/46399096/d3526dfe-75c9-4cd0-8cab-eb49abcc3404" width = "35%" alt=""/>
      <br>
      <div style="color:orange; border-bottom: 1px solid #d9d9d9;
      display: inline-block;
      color: #999;
      padding: 2px;">
        Fig4: LoRA è·¨åŸŸæ¢¯åº¦ä¿¡æ¯å­¦ä¹ åˆ†æ”¯ä¸åŸå§‹å…¨å‚æ¨¡å‹çš„è¿æ¥æ–¹å¼
        </div>
  </center>
  
  <center>
      <img style="border-radius: 0.3125em;
      box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
      src="https://github.com/Tsaiyue/tsaiyue.github.io/assets/46399096/489a4c8d-670f-4fa9-80bd-63a80738fa44" width = "45%" alt=""/>
      <br>
      <div style="color:orange; border-bottom: 1px solid #d9d9d9;
      display: inline-block;
      color: #999;
      padding: 2px;">
        Fig4: å‰å‘æ¨ç†æ–¹å¼
        </div>
  </center>

#### ç›¸å…³å®ç°ï¼š

- huggingface/diffusersæˆ–PaddleMIX/ppdiffusersä¸­åŒ…å«äº†ç›¸å…³LoRAå¤„ç†æ³¨æ„åŠ›å‚æ•°çš„æ¥å£ï¼Œé€šè¿‡æ•è·æ¨¡å‹å¯¹åº”å‚æ•°ï¼Œä¸ºå…¶é…ç½®LoRAæ¢¯åº¦å­¦ä¹ åˆ†æ”¯ï¼š
  
  ```python
  # Set correct lora layers
  unet_lora_attn_procs = {}
  for name, attn_processor in unet.attn_processors.items():
      cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
      if name.startswith("mid_block"):
          hidden_size = unet.config.block_out_channels[-1]
      elif name.startswith("up_blocks"):
          block_id = int(name[len("up_blocks.")])
          hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
      elif name.startswith("down_blocks"):
          block_id = int(name[len("down_blocks.")])
          hidden_size = unet.config.block_out_channels[block_id]
  
      unet_lora_attn_procs[name] = LoRAAttnAddedKVProcessor(
          hidden_size=hidden_size,
          cross_attention_dim=cross_attention_dim,
          rank=args.lora_rank,
         )
  
  unet.set_attn_processor(unet_lora_attn_procs)
  unet_lora_layers = AttnProcsLayers(unet.attn_processors)
  ```

- Huggingface/peftæä¾›äº†æ›´é«˜å±‚çš„å®ç°ï¼ŒåŸºäº`get_peft_model`å’ŒÂ `get_peft_config`Â Â åŸºäºLoRAç›¸å…³configæ–‡ä»¶è¿›è¡Œå¤„ç†ï¼š
  
  ```python
  from transformers import AutoModelForSeq2SeqLM
  from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
  model_name_or_path = "bigscience/mt0-large"
  tokenizer_name_or_path = "bigscience/mt0-large"
  
  peft_config = LoraConfig(
      task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
  )
  
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
  model = get_peft_model(model, peft_config)
  model.print_trainable_parameters()
  ```

#### è®¨è®ºï¼š

- é’ˆå¯¹Transformerçš„çº¿æ€§å±‚ï¼Œä½†ç†è®ºä¸Šå¯é’ˆå¯¹ä»»ä½•ç¥ç»ç½‘ç»œæ¨¡å‹ï¼›

- è®­ç»ƒå¾®è°ƒå‚æ•°é‡ç›¸è¾ƒäºFTå¯é™ä½10000å€ï¼Œä½†æ¨ç†ç”±äºå¼•å…¥æ¢¯åº¦å­¦ä¹ åˆ†æ”¯å°†å¸¦æ¥ä¸€å®šçš„æ¨ç†å»¶è¿Ÿï¼Œnon-straightforwardï¼›

- å¯ä¸å…¶ä»–é«˜æ•ˆå¾®è°ƒæ–¹å¼ç»“åˆï¼Œå¦‚Prefix-Tuningï¼›

- å…³äºæ¢¯åº¦åˆ†æ”¯å‚æ•°çš„åˆå§‹åŒ–è®¾ç½®ï¼Œdecoderå¯ä»¥ç”¨0åˆå§‹åŒ–ï¼Œä½†encoderä¸èƒ½ï¼Œå¦åˆ™å°†æ— æ³•è¿›è¡Œæ¢¯åº¦æ›´æ–°.

#### å‚è€ƒ

- [1] LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS, 2021, microsoft, [[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

- [2] [GitHub - huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)


